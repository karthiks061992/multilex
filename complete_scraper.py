# -*- coding: utf-8 -*-
"""money_control_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17jYHPZxdt-2t_Mon96D7lK0KIkXfMhFQ
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup

#Money control scraper starts
articles=[]
links=[]
dates=[]
url="https://www.moneycontrol.com/news/business/ipo/page-1"
pg=requests.get(url)
sp=BeautifulSoup(pg.content,'html.parser')
for i in range(1,len(sp.find_all('h2'))):
  pdd=sp.find_all('h2')[i]
  articles.append(sp.find_all('h2')[i].get_text())
  for a in pdd.find_all('a',href=True):
    links.append(a["href"])
  dates.append(sp.find_all('span')[137+i].get_text())

orgs=[]
for i in range(0,len(articles)-1):
  if articles[i]=='' or articles[i]=='Watch':
    articles.pop(i)
    #links.pop(i)
    dates.pop(i)
for i in range(0,len(dates)-1):
  if dates[i]=='' or dates[i]=='Â«':
    dates.pop(i)
for i in range(0,len(articles)-1):
  orgs.append('orgs')

df1 = pd.DataFrame(list(zip(articles,links,dates,orgs)), 
               columns =['Text', 'Links','Dates','Organization'])

df1# df1 resembles money_control scraper and end of it

#Start of rss feed scraper from IPO wire
URL="https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe"
page=requests.get(URL)
soup=BeautifulSoup(page.content,features='xml')
articles=[]
links=[]
dates=[]
org=[]
for i in range(1,len(soup.find_all('title'))):
  pdd=soup.find_all('title')[i]
  if soup.find_all('title')[i].get_text() not in articles:
    articles.append(soup.find_all('title')[i].get_text())
    links.append(soup.find_all('link')[i].get_text())
    dates.append(soup.find_all('pubDate')[i-1].get_text())
    org.append('ORGS')

df2 = pd.DataFrame(list(zip(articles,links,dates,org)), 
               columns =['Text', 'Links','Dates','Organization']) 
#Rss feed scraper ends

#Start of non rss feed scraper by IPO wire
articles=[]
links=[]
dates=[]
for j in range(1,50):
  URL="https://ipo.einnews.com/?page="+str(j)
  page=requests.get(URL)
  soup=BeautifulSoup(page.content,'html.parser')
  for i in range(1,len(soup.find_all('h3'))-31):
    if soup.find_all('h3')[i].get_text() not in articles:
      pdd=soup.find_all('h3')[i]
      articles.append(soup.find_all('h3')[i].get_text())
      for a in pdd.find_all('a',href=True):
        links.append(a["href"])
      dates.append(soup.find_all('span',{'class':'date'})[i].get_text())

cleaned_articles=[]
import re
for article in articles:
  cleanString = re.sub('\n','', article)
  cleaned_articles.append(cleanString)
#len(cleaned_articles)

orgs=[]
for i in range(0,len(cleaned_articles)):
  orgs.append('orgs')
df3_non_updated1= pd.DataFrame(list(zip(cleaned_articles,links,dates,orgs)), 
               columns =['Text', 'Links','Dates','Organization'])

articles=[]
links=[]
dates=[]
orgs=[]
keywords=['IPO','ipo','going','public','pre-ipo','PRE-IPO','ipo,','IPO,']
flag=False
for i in range(0,df3_non_updated1.shape[0]):
  x=df3_non_updated1['Text'][i]
  x=x.split(" ")
  for let in x:
    if let in keywords:
      flag=True
  if flag==True:
    articles.append(df3_non_updated1['Text'][i])
    links.append(df3_non_updated1['Links'][i])
    dates.append(df3_non_updated1['Dates'][i])
    orgs.append('orgs')
    flag=False

df3_non_updated2= pd.DataFrame(list(zip(articles,links,dates,orgs)), 
               columns =['Text', 'Links','Dates','Organization'])

df3_non_updated2
for i in range(0,df3_non_updated2.shape[0]):
  if df3_non_updated2['Links'][i][0]=='/':
    df3_non_updated2['Links'][i]='http://ipo.einnews.com'+str(df3_non_updated2['Links'][i])

df3=df3_non_updated2
df3
#End of non rss feed scraper by IPO wire

frames=[df1,df2,df3]

result=pd.concat(frames)
result