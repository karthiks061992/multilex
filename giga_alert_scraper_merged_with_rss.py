# -*- coding: utf-8 -*-
"""giga_alert_scraper_merged_with_rss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZK12KA_jQBAwbr5ezUNLymmb8rNH-uk7
"""

import requests
from bs4 import BeautifulSoup

#Giga alerts scraper starts
url="http://www.gigaalert.com/browse.php?u=jbloch&c=b917d3&s=all&t=&r=p&a="
page=requests.get(url)
soup=BeautifulSoup(page.content,'html.parser')

art_link={}
art_link1={}
for i in range(1,len(soup.find_all('a'))):
  art_link[soup.find_all('a')[i].get_text()]=soup.find_all('a')[i].get('href')
for key in art_link:
  if key.isnumeric() or key=='' or key=='\xa0All\xa0' or key=='\nFrames\n' or key=='this is relevant' or key=='irrelevant' or key=='Terms of use' or key=='All' or key=='Search' or key=="unmark":
    continue
  else:
    art_link1[key]=art_link[key]

from datetime import datetime

# datetime object containing current date and time
now = datetime.now()
#art_link1
dates=[]
orgs=[]
for key in art_link1:
  dates.append(now)
  orgs.append('orgs')

import pandas as pd
df=pd.DataFrame(columns=['Text','Links','Dates','Organization'])
articles=[]
links=[]
for key in art_link1:
  articles.append(key)
  links.append(art_link1[key])
df = pd.DataFrame(list(zip(articles,links,dates,orgs)), 
               columns =['Text', 'Links','Dates','Organization'])

df

articles=[]
links=[]
dates=[]
orgs=[]
for i in range(0,df.shape[0]):
  if df['Text'][i][0].isnumeric():
    continue
  else:
    articles.append(df['Text'][i])
    links.append(df['Links'][i])
    dates.append(now)
    orgs.append('orgs')
df = pd.DataFrame(list(zip(articles,links,dates,orgs)), 
               columns =['Text', 'Links','Dates','Organization'])

df

for i in range(0,df.shape[0]):
  
  strs1='http://gigaalerts.com/'+df['Links'][i]
  df['Links'][i]=strs1

df
#giga alerts scraper ends as dataframe df

#RSS feed scrapper starts
URL="https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe"
page=requests.get(URL)
soup=BeautifulSoup(page.content,features='xml')

import html
articles=[]
links=[]
dates=[]
description=[]
org=[]
for i in range(1,len(soup.find_all('title'))):
  pdd=soup.find_all('title')[i]
  if soup.find_all('title')[i].get_text() not in articles:
    articles.append(soup.find_all('title')[i].get_text())
    links.append(soup.find_all('link')[i].get_text())
    dates.append(soup.find_all('pubDate')[i-1].get_text())
    description.append(html.unescape(soup.find_all('description')[i].get_text()))
    org.append('ORGS')

#cleaning description
cleaned_description=[]
import re
for i in range(0,len(description)-1):
  art=description[i]
  cleaned_description.append(re.sub('\n','',art))

df1 = pd.DataFrame(list(zip(articles,links,dates,org,cleaned_description)), 
               columns =['Text', 'Links','Dates','Organization','description'])

import spacy
nlpq = spacy.load('en_core_web_sm')
for i in range(0,df1.shape[0]):
  article=df1['Text'][i]
  doc=nlpq(article)
  for ele in doc.ents:
    if ele.label_=='ORG':
      df1['Organization'][i]=ele
    else:
      df1['Organization'][i]="organization not found"

import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now() 
print(current_time)
for i in range(0,df1.shape[0]):
  df1['Dates'][i]=current_time

#df1
df1=df1.drop(columns=['description'])
df1
#RSS feed scrapper ends

#Merging both the scrapers
frames=[df,df1]
result_df=pd.concat(frames)

result_df

from google.colab import drive
drive.mount('drive')

result_df.to_csv('21st_final_merge.csv')
!cp 21st_final_merge.csv "drive/My Drive/"