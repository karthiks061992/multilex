# -*- coding: utf-8 -*-
"""rss_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uS7JcxCNByf9I0Ga25Td1CvTOFjlPq36
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup

URL="https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe"
page=requests.get(URL)
soup=BeautifulSoup(page.content,features='xml')

print(soup.prettify())

import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now() 
import html
articles=[]
links=[]
dates=[]
description=[]
org=[]
scraped_date=[]
for i in range(1,len(soup.find_all('title'))):
  pdd=soup.find_all('title')[i]
  if soup.find_all('title')[i].get_text() not in articles:
    articles.append(soup.find_all('title')[i].get_text())
    links.append(soup.find_all('link')[i].get_text())
    dates.append(soup.find_all('pubDate')[i-1].get_text())
    description.append(html.unescape(soup.find_all('description')[i].get_text()))
    org.append('ORGS')
    scraped_date.append(current_time)

#cleaning description
cleaned_description=[]
import re
for i in range(0,len(description)-1):
  art=description[i]
  cleaned_description.append(re.sub('\n','',art))

cleaned_description



df1 = pd.DataFrame(list(zip(articles,links,dates,org,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date'])

scraped_date=[]
import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now() 
print(current_time)
for i in range(0,df1.shape[0]):
  #df1[''][i]=current_time
  scraped_date.append(current_time)

import spacy
nlpq = spacy.load('en_core_web_sm')
for i in range(0,df1.shape[0]):
  article=df1['Text'][i]
  doc=nlpq(article)
  for ele in doc.ents:
    if ele.label_=='ORG':
      df1['Organization'][i]=ele
    else:
      df1['Organization'][i]="organization not found"

df1

df1['Text'][37]

rep_date=df1['Dates'][0]
rep_date=rep_date.split(' ')
rep_date=rep_date[1]
rep_date

import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now()            
tod_date=str(current_time.day)

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
for i in range(0,df1.shape[0]):
  rep_date=df1['Dates'][i]
  rep_date=rep_date.split(' ')
  rep_date=rep_date[1]
  if rep_date==tod_date:
    articles.append(df1['Text'][i])
    links.append(df1['Links'][i])
    dates.append(df1['Dates'][i])
    orgs.append(df1['Organization'][i])
    scraped_date.append(df1['scraped_date'][i])

df1 = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date'])

df1

for i in range(0,df1.shape[0]):
  lin=df1['Links'][i]
  r=requests.get(lin)
  df1['Links'][i]=r.url

df1

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
url="https://www.google.co.in/alerts/feeds/15296043414695393299/12391429027627390948"
page=requests.get(url)
soup=BeautifulSoup(page.content,features='xml')
#print(soup.prettify())
for i in range(0,len(soup.find_all('entry'))):
  pdd=soup.find_all('entry')[i]
  for title in pdd.find_all('title'):
    articles.append(html.unescape(title.text))
  for link in pdd.find_all('link'):
    links.append(html.unescape(link['href']))
  for date in pdd.find_all('published'):
    dates.append(date.text)
    orgs.append('orgs')
    scraped_date.append(date.text)

df_google = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date'])

df_google

for i in range(0,df_google.shape[0]):
  lin=df_google['Links'][i]
  r=requests.get(lin)
  df_google['Links'][i]=r.url

df_google

df_final=pd.concat([df1,df_google])

df_final=df_final.reset_index()

df_final=df_final.drop(columns=['index'])
df_final

from google.colab import drive
drive.mount('drive')

df_final.to_csv('rss_28th.csv')
!cp rss_28th.csv "drive/My Drive/"