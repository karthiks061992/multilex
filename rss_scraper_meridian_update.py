# -*- coding: utf-8 -*-
"""rss_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uS7JcxCNByf9I0Ga25Td1CvTOFjlPq36
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup

URL="https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe"
page=requests.get(URL)
soup=BeautifulSoup(page.content,features='xml')

print(soup.prettify())

import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now() 
import html
articles=[]
links=[]
dates=[]
description=[]
org=[]
scraped_date=[]
for i in range(1,len(soup.find_all('title'))):
  pdd=soup.find_all('title')[i]
  if soup.find_all('title')[i].get_text() not in articles:
    articles.append(soup.find_all('title')[i].get_text())
    links.append(soup.find_all('link')[i].get_text())
    dates.append(soup.find_all('pubDate')[i-1].get_text())
    description.append(html.unescape(soup.find_all('description')[i].get_text()))
    org.append('ORGS')
    scraped_date.append(current_time)

#cleaning description
cleaned_description=[]
import re
for i in range(0,len(description)-1):
  art=description[i]
  cleaned_description.append(re.sub('\n','',art))

cleaned_description



df1 = pd.DataFrame(list(zip(articles,links,dates,org,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date'])

scraped_date=[]
import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now()
current_time=current_time.strftime("%Y-%m-%d %H:%M:%S")
print(current_time) 
for i in range(0,df1.shape[0]):
  #df1[''][i]=current_time
  scraped_date.append(current_time)

import spacy
nlpq = spacy.load('en_core_web_sm')
for i in range(0,df1.shape[0]):
  article=df1['Text'][i]
  doc=nlpq(article)
  for ele in doc.ents:
    if ele.label_=='ORG':
      df1['Organization'][i]=ele
    else:
      df1['Organization'][i]="organization not found"

df1

rep_date=df1['Dates'][0]
rep_date=rep_date.split(' ')
rep_date=rep_date[1]
rep_date

import datetime  
    
# using now() to get current time  
current_time = datetime.datetime.now()            
tod_date=str(current_time.day)
if len(tod_date)==1:
  tod_date='0'+tod_date
  tod_date=str(tod_date)
print(tod_date)

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
for i in range(0,df1.shape[0]):
  rep_date=df1['Dates'][i]
  rep_date=rep_date.split(' ')
  rep_date=rep_date[1]
  if rep_date==tod_date:
    articles.append(df1['Text'][i])
    links.append(df1['Links'][i])
    dates.append(df1['Dates'][i])
    orgs.append(df1['Organization'][i])
    scraped_date.append(df1['scraped_date'][i])

df1 = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date']) 
df1

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
flag=False
keywords=['IPO','IPO','IPO ','SPACs','ipo','pre-IPO','pre-ipo','PRE-IPO','pre-IPO','going public','public','closes','listing','planning','closing','excellent','Public','Initial','Offering','initial','Announces','Pricing','pricing','announces','launches','Launches','SPAC','spac']
for i in range(0,df1.shape[0]):
  article=df1['Text'][i]
  article=article.split(" ")
  for let in article:
    if let in keywords:
      flag=True
  if flag==True:
    articles.append(df1['Text'][i])
    links.append(df1['Links'][i])
    dates.append(df1['Dates'][i])
    orgs.append(df1['Organization'][i])
    scraped_date.append(df1['scraped_date'][i])
    flag=False

df1 = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date']) 
df1



# for i in range(0,df1.shape[0]):
#   r=requests.get(df1['Links'][i])
#   df1['Links'][i]=r.url

df1

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
url="https://www.google.co.in/alerts/feeds/15296043414695393299/12391429027627390948"
page=requests.get(url)
soup=BeautifulSoup(page.content,features='xml')
#print(soup.prettify())
for i in range(0,len(soup.find_all('entry'))):
  pdd=soup.find_all('entry')[i]
  for title in pdd.find_all('title'):
    articles.append(html.unescape(title.text))
  for link in pdd.find_all('link'):
    links.append(html.unescape(link['href']))
  for date in pdd.find_all('published'):
    dates.append(date.text)
    orgs.append('orgs')
    scraped_date.append(date.text)

df_google = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date'])

df_google

articles=[]
links=[]
dates=[]
orgs=[]
scraped_date=[]
for i in range(0,df_google.shape[0]):
  date=df_google['Dates'][i]
  date=date.split('-')
  date=date[2]
  combined_date=str(date[0]+date[1])
  if combined_date==tod_date:
    articles.append(df_google['Text'][i])
    links.append(df_google['Links'][i])
    dates.append(df_google['Dates'][i])
    orgs.append('orgs')
    scraped_date.append(df_google['scraped_date'][i])

df_google = pd.DataFrame(list(zip(articles,links,dates,orgs,scraped_date)), 
               columns =['Text', 'Links','Dates','Organization','scraped_date']) 
df_google

df_final=pd.concat([df1,df_google])
df_final



df_final=df_final.reset_index()

df_final=df_final.drop(columns=['index'])
df_final

df_final

for i in range(0,df_final.shape[0]):
  try:
    response=requests.get(df_final['Links'][i])
    df_final['Links'][i]=response.url
  except:
    continue

df_final





# from google.colab import drive
# drive.mount('drive')

# df_final.to_csv('rss_1st_10am.csv')
# !cp rss_30th_10am.csv "drive/My Drive/"